{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "MXNet supports automatic differentiation with the `autograd` package.\n",
    "`autograd` allows you to differentiate a graph of NDArray operations\n",
    "with the chain rule.\n",
    "This is called define-by-run, i.e., the network is defined on-the-fly by\n",
    "running forward computation. You can define exotic network structures\n",
    "and differentiate them, and each iteration can have a totally different\n",
    "network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `autograd`, we must first mark variables that require gradient and\n",
    "attach gradient buffers to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mx.nd.array([[1, 2], [3, 4]])\n",
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the network while running forward computation by wrapping\n",
    "it inside a `record` (operations out of `record` does not define\n",
    "a graph and cannot be differentiated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "  y = x * 2\n",
    "  z = y * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's backprop with `z.backward()`, which is equivalent to\n",
    "`z.backward(mx.nd.ones_like(z))`. When z has more than one entry, `z.backward()`\n",
    "is equivalent to `mx.nd.sum(z).backward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's see if this is the expected output.\n",
    "\n",
    "Here, y = f(x), z = f(y) = f(g(x))\n",
    "which means y = 2 * x and z = 2 * x * x.\n",
    "\n",
    "After, doing backprop with `z.backward()`, we will get gradient dz/dx as follows:\n",
    "\n",
    "dy/dx = 2,\n",
    "dz/dx = 4 * x\n",
    "\n",
    "So, we should get x.grad as an array of [[4, 8],[12, 16]].\n",
    "\n",
    "<!-- INSERT SOURCE DOWNLOAD BUTTONS -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "display_name": "",
  "language": "python",
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
